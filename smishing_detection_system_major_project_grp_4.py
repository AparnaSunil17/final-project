# -*- coding: utf-8 -*-
"""Smishing Detection system-Major project Grp 4

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1snc03x64YCqv0GrSEKJ-UaRtzMTGm7Ak
"""

!pip install datasets

# BEST 23/3/25 - 12:10 PM (Modified with SHAP Summary)
import re
import os
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import shap
import transformers
from urllib.parse import urlparse
from sklearn.ensemble import AdaBoostClassifier, ExtraTreesClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments
from datasets import Dataset
from shap.maskers import Text
from IPython.core.display import display, HTML

# ✅ Enable GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# ------------------------------------------
# 🔹 Step 1: URL-Based Phishing Detection (ABET)
# ------------------------------------------

def extract_features(url):
    """Extracts relevant features from a URL."""
    parsed_url = urlparse(url)
    domain = parsed_url.netloc.lower()

    return {
        "url_length": len(url),
        "num_digits": sum(c.isdigit() for c in url),
        "num_special_chars": len(re.findall(r"[!@#$%^&*(),.?\":{}|<>]", url)),
        "domain_length": len(domain),
        "num_subdomains": domain.count("."),
        "has_https": int(url.startswith("https")),
        "has_ip_address": int(bool(re.search(r'\d+\.\d+\.\d+\.\d+', domain))),
        "num_hyphens": domain.count("-"),
        "num_slashes": url.count("/"),
        "num_query_params": url.count("="),
        "has_suspicious_words": int(any(word in url for word in ["login", "bank", "verify", "secure", "update", "account"]))
    }

# Load dataset
file_path = "dataset_final.csv"  # Update with actual dataset path
df = pd.read_csv(file_path, encoding="utf-8")
df['URL'] = df['URL'].fillna('NIL')

# Extract full feature set
url_features = pd.DataFrame(df["URL"].apply(extract_features).tolist())
y_url = df['Label_url'].astype(int)

# Train-Test Split
X_train_url, X_test_url, y_train_url, y_test_url = train_test_split(url_features, y_url, test_size=0.2, random_state=42)

# ✅ Train AdaBoost with Extra Trees
adaboost_model = AdaBoostClassifier(
    estimator=ExtraTreesClassifier(n_estimators=100, max_depth=8, random_state=42),
    n_estimators=50, learning_rate=0.05, random_state=42
)
adaboost_model.fit(X_train_url, y_train_url)

# Evaluate Model
y_pred_url = adaboost_model.predict(X_test_url)
print(f"\n🔹 URL Classifier Accuracy: {accuracy_score(y_test_url, y_pred_url) * 100:.2f}%")
print(classification_report(y_test_url, y_pred_url))

# ------------------------------------------
# 🔹 Step 2: Fine-Tuning DistilBERT for Smishing
# ------------------------------------------

MODEL_NAME = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

if 'Message' in df.columns and 'Label_msg' in df.columns:
    sms_df = df[['Message', 'Label_msg']].dropna()

    def tokenize_function(example):
        return tokenizer(example["Message"], padding="max_length", truncation=True, max_length=256)

    dataset = Dataset.from_pandas(sms_df)
    dataset = dataset.map(tokenize_function, batched=True)
    dataset = dataset.rename_column("Label_msg", "labels")
    dataset = dataset.remove_columns(["Message"])

    train_test_split = dataset.train_test_split(test_size=0.2)
    train_dataset = train_test_split["train"]
    test_dataset = train_test_split["test"]

    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2).to(device)

    training_args = TrainingArguments(
        output_dir="./distilbert_model",
        evaluation_strategy="epoch",
        save_strategy="epoch",
        learning_rate=2.796e-5,
        per_device_train_batch_size=8,
        per_device_eval_batch_size=8,
        num_train_epochs=4,
        weight_decay=0.264,
        logging_dir="./logs",
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=test_dataset,
    )

    trainer.train()
    results = trainer.evaluate()
    print("\n🔹 DistilBERT SMS Model Evaluation:", results)

else:
    print("\n⚠ SMS message dataset not found in 'dataset_final.csv'.")

# ------------------------------------------
# 🔹 Step 3: Unified Real-Time Analysis (URL + Message)
# ------------------------------------------

def predict_url(url):
    """Predict if the given URL is phishing or safe."""
    url_features = extract_features(url)
    url_vector = pd.DataFrame([url_features])
    phishing_prob = adaboost_model.predict_proba(url_vector)[0][1]
    return phishing_prob

def predict_message(text):
    """Predict if the given message is phishing or ham."""
    inputs = tokenizer(text, padding=True, truncation=True, return_tensors="pt").to(device)
    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.nn.functional.softmax(outputs.logits, dim=-1).cpu().numpy()
    return probs[0][1]

# ✅ SHAP Masker & Explainer
masker = Text(tokenizer)
explainer = shap.Explainer(lambda x: np.array([predict_message(t) for t in x]), masker)

def highlight_shap_text(shap_values, text):
    """Highlights words in red (phishing-related) and blue (safe)."""
    words = text.split()
    shap_vals = shap_values.values[0]

    colored_text = []
    for word, val in zip(words, shap_vals):
        color = "red" if val > 0 else "blue"
        colored_text.append(f'<span style="color:{color}; font-weight:bold;">{word}</span>')

    return " ".join(colored_text)

def get_suspicious_words(shap_values, text, threshold=0.1, top_k=3):
    """Return suspicious words based on SHAP values and threshold."""
    words = text.split()
    shap_vals = shap_values.values[0]

    # Get words with SHAP value > threshold
    suspicious = [(word, val) for word, val in zip(words, shap_vals) if val > threshold]

    # If not enough, pick top-k SHAP values
    if len(suspicious) < top_k:
        top_k_vals = sorted(zip(words, shap_vals), key=lambda x: -x[1])[:top_k]
        suspicious_words = [w for w, _ in top_k_vals]
    else:
        suspicious_words = [w for w, _ in suspicious]

    return list(dict.fromkeys(suspicious_words))  # remove duplicates, preserve order

def classify_input(user_input):
    """Extracts and classifies URLs and messages separately."""
    urls = re.findall(r"https?://\S+|www\.\S+", user_input)
    message = re.sub(r"https?://\S+|www\.\S+", "", user_input).strip()

    url_probs = [predict_url(url) for url in urls] if urls else [0]
    message_prob = predict_message(message) if message else 0

    # 🏆 Final Combined Phishing Probability
    final_prob = (0.7 * max(url_probs)) + (0.3 * message_prob)
    label = "🚨 PHISHING" if final_prob > 0.5 else "✅ SAFE"
    print(f"\n🔹 FINAL DECISION: {label} (Combined Probability: {final_prob:.4f})")

    if urls:
        print("\n🔹 URL Analysis:")
        for url, prob in zip(urls, url_probs):
            print(f"🔗 {url} → {'🚨 PHISHING' if prob > 0.5 else '✅ SAFE'} (Probability: {prob:.4f})")

    if message:
        print(f"\n📝 Message Analysis: {'🚨 PHISHING' if message_prob > 0.5 else '✅ SAFE'} (Probability: {message_prob:.4f})")

        # 🔥 SHAP Highlighted Explanation
        shap_values = explainer([message])
        highlighted_text = highlight_shap_text(shap_values, message)

        print("\n📊 SHAP Explanation for Message:")
        display(HTML(f"<p style='font-size:16px;'>{highlighted_text}</p>"))

        # ✅ SHAP Summary Message
        suspicious_words = get_suspicious_words(shap_values, message)
        if suspicious_words:
            print(f"\n📌 SHAP Explanation Summary: Highlighted words like {', '.join(f'‘{w}’' for w in suspicious_words)} as suspicious.")
        else:
            print("\n✅ SHAP Explanation Summary: No highly suspicious words detected.")

# ------------------------------------------
# 🔹 Real-Time Input
# ------------------------------------------
user_input = input("\nEnter a URL and/or message to classify: ")
classify_input(user_input)